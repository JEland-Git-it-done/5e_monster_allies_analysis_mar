from __future__ import absolute_import, division, print_function
import pandas as pd; import numpy as np;
import requests; import os.path; import re
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation; from heapq import nlargest
from gensim.summarization import summarize, keywords

from bs4 import BeautifulSoup



#Alternative implementation taken from here https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/
def alternate_summary(title, text):
    text = text.replace("[^a-zA-Z0-9\s]", "")
    print(text)
    print(type(STOP_WORDS))
    stopwords = list(STOP_WORDS) #Convert Set to list
    nlp = spacy.load('en_core_web_sm')
    doc_nlp = nlp(text)
    #for token in docx: Test case of text output
    #   print(token.text)
    tokens = [token.text for token in doc_nlp]
    word_freq = {}
    for word in doc_nlp:
        if word.text not in stopwords:
            if word.text not in word_freq.keys():
                word_freq[word.text] = 1
            else:
                word_freq[word.text] += 1
    print(word_freq)
    max_freq = max(word_freq.values())
    for word in word_freq.keys():
        word_freq[word] = (word_freq[word]/max_freq) #creates sub 0 weights
    sent_list = [sent for sent in doc_nlp.sents]
    sent_score = dict()
    for sent in sent_list:
        for word in sent:
            if word.text.lower() in word_freq.keys():
                if len(sent.text.split(" ")) < 25:
                    if sent not in sent_score.keys():
                        sent_score[sent] = word_freq[word.text.lower()]
                    else:
                        sent_score[sent] += word_freq[word.text.lower()]
    #print(sent_score)
    g = round(len(sent_list) * 0.55) #Variable that changes to ensure the summary is atleast
    #using a number of sentances equal to 30% of the overall sentance size of main article
    #print(g)
    summ_sentances = nlargest(g, sent_score, key=sent_score.get)
    summ_output_sents = [word.text for word in summ_sentances]
    summ = " ".join(summ_output_sents)
    print("Keywords relating to {} ('s) are \n".format(title), keywords(text, ratio=0.08).split("\n"))
    print("This is a summary of combat tactics and information, generated by spaCy, relating to: {} \n".format(title), summ)
    print("\n\n\n", "This is an alternative summary, generated by gensim: \n", summarize(text, ratio=0.6))

    

def read_blogs():
    if os.path.exists("articles.xlsx"):
        print("Using preexisting excell sheet instead")
        df = pd.read_excel("articles.xlsx", index_col=0)
        print(df)
        for i in range(5):
            print("*")
        return df

    else:
        page_urls = ["cr-1-4", "cr-1-2"]
        print("Reading blogs this may take a while.")
        url_list, entry_dict = [], {} #list will be passed to another function to read sub pages
        for i in range(21): #Max value of CR
            page_urls.append("cr-{}".format(i+1)) #Automating target pages
        print(page_urls)

        for i in range(len(page_urls)): #Is currently set to the first 6 entries for speed
            address = "http://themonstersknow.com/tag/{}/".format(page_urls[i])
            file = requests.get(address)
            print(address)
            soup = BeautifulSoup(file.content, "html.parser")
            read_data = soup.find_all("div", class_="entry-content")
            #Needs to follow link found in <a > tag, so that complete page is shown
            b = 0 #Local Iterator
            for item in read_data:
                b = b + 1; x = b
                print(x, item.get_text(), "\n ", item.a)
                if item.a is None:
                    text_entry = item.get_text()
                    text_entry = text_entry.replace("\n", "")
                    entry_dict[str(item.strong.text)] =  text_entry #This should make keys and values that work without the need to manually assign them

                elif item.a is not None:
                    print(item.a.get("href"))
                    if "http://" not in item.a.get("href"):
                        pass
                    else:
                        url_list.append(item.a.get("href"))
        print("Reading sub lists")
        url_list, entry_dict = read_sublinks(url_list, entry_dict)

        url_list = list(dict.fromkeys(url_list))
        print("Testing: {}".format(url_list), "\n ")
        print("formed dictionary of articles ", entry_dict.keys())
        df = form_df(entry_dict)
        return df

def form_df(entry_dict):
    df_mk = pd.DataFrame(columns=["article_id", "text"])
    for k, v in sorted (entry_dict.items()):
        print(k, "is the key for: ", v)
        df_mk = df_mk.append({"article_id": k, "text": v}, ignore_index=True)
    form_sql(df_mk)
    df_mk.to_excel("articles.xlsx", sheet_name="monstersknow")
    return df_mk

def clean_text(df_targ):
    article = str(df_targ["text"].values)
    sections = article.split("Next: ")
    text = str(sections[0])
    return text

def find_article():
    df = read_blogs()
    df = df.sample(1)
    target_text = clean_text(df)
    title = str(df["article_id"].values)
    output = [title, target_text]
    #print(target_text)
    return output



find_article()
arg = find_article()
alternate_summary(title=arg[0], text=arg[1])